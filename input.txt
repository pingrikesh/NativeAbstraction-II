20BiliScreen: Smartphone-Based Scleral Jaundice Monitoring for Liver and Pancreatic DisordersALEX MARIAKAKIS, MEGAN A. BANKS, LAUREN PHILLIPI, LEI YU, JAMES TAYLOR, and SHWE-TAK N. PATEL,University of WashingtonPancreatic cancer has one of the worst survival rates amongst all forms of cancer because its symptoms manifest later intothe progression of the disease. One of those symptoms is jaundice, the yellow discoloration of the skin and sclera due to thebuildup of bilirubin in the blood. Jaundice is only recognizable to the naked eye in severe stages, but a ubiquitous test usingcomputer vision and machine learning can detect milder forms of jaundice. We propose BiliScreen, a smartphone app thatcaptures pictures of the eye and produces an estimate of a person’s bilirubin level, even at levels normally undetectable by thehuman eye. We test two low-cost accessories that reduce the effects of external lighting: (1) a 3D-printed box that controls theeyes’ exposure to light and (2) paper glasses with colored squares for calibration. In a 70-person clinical study, we found thatBiliScreen with the box achieves a Pearson correlation coefficient of 0.89 and a mean error of -0.09±2.76 mg/dl in predictinga person’s bilirubin level. As a screening tool, BiliScreen identifies cases of concern with a sensitivity of 89.7% and a specificityof 96.8% with the box accessory.CCS Concepts: •Human-centered computing?Smartphones;•Applied computing?Consumer health;Additional Key Words and Phrases: Health sensing; smartphones; jaundice; bilirubin; image processingACMReferenceformat:AlexMariakakis,MeganA.Banks,LaurenPhillipi,LeiYu,JamesTaylor,andShwetakN.Patel.2017.BiliScreen:Smartphone-BasedScleralJaundiceMonitoringforLiverandPancreaticDisorders.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.1,2,Article20(June 2017),26pages.DOI: http://doi.org/10.1145/30900851 INTRODUCTIONAmong all forms of cancer, Pancreatic cancer has one of the worst survival rates [2]. Many attribute this statisticto the fact that the symptoms associated with pancreatic cancer often go unnoticed until the cancer is in a laterstage; 80-85% of patients present themselves with tumors so advanced that they cannot be removed completelythrough surgery [5,34]. One of the earliest symptoms to appear is jaundice, a yellow discoloration of the skinand eyes. In the case of pancreatic cancer, jaundice occurs because a cancerous growth obstructs the common bileduct, causing a buildup of bilirubin in the blood [11]. Being able to detect the very first signs of jaundice whenlevels of bilirubin are minimally elevated could enable an entirely new screening program for at-risk individuals.Jaundice also manifests as a symptom for a variety of other conditions, such as hepatitis and Gilbert’s syndrome,but we are primarily motivated by the link between jaundice and pancreatic cancer for the purpose of this paper.ThisworkissupportedbytheNationalScienceFoundationGraduateResearchFellowshipProgramandtheCoulterFoundation.Author’saddresses:A.MariakakisandM.A.BanksandS.N.Patel,ComputerScienceandEngineeringDepartment,UniversityofWashington;L.PhillipiandL.Yu,UniversityofWashingtonMedicalCenter;J.Taylor,DepartmentofPediatrics,UniversityofWashington.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.©2017AssociationforComputingMachinery.2474-9567/2017/-ART20$15.00DOI: http://doi.org/10.1145/3090085Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 2, Article 20. Publication date: June 2017.
BiliScreen: Smartphone-Based Scleral Jaundice Monitoring for Liver and Pancreatic Disorders • 20:7Fig. 2.(top)A 3D rendering of the BiliScreen box. The smartphone’s flash lies in the horizontal center of the box. The flashis covered with a neutral density filter and a diffuser to make the light more comfortable.(bottom)A rendering of theBiliScreen glasses.3.3 BiliScreen AccessoriesPhysics-based models for color information typically consider an object’s visible color to be the combinationof two components: a body reflection component, which describes the object’s color, and a surface reflectioncomponent, which describes the incident illuminant [15]. When using digital photography, color informationthat gets stored in image files is also impacted by the camera sensor’s response to different wavelengths. For ourstudy, we examine the efficacy of two different accessories to isolate the sclera’s body reflection component indifferent ways (Fig. 2).The first accessory is a 3D-printed box reminiscent of a Google Cardboard headset2. There is no electricalconnection between the phone and the box; the phone is simply slid into the box via a rectangular channel alongthe back. The channel at the back of the box also fixes the placement of the phone relative to the participant’s faceby centering the phone’s camera and keeping it at a fixed distance. The box blocks out ambient lighting whileallowing the phone’s flash to provide the only illumination onto the eyes. From pilot studies, some participantsfound the flash to be overwhelmingly bright. A neutral density filter and a diffuser were placed in front of theflash using a filter holder to soften the light slightly. The box used in our study was 3D-printed, but it could bemade with an even cheaper material like cardboard (provided that it is sturdy enough to support the weight ofthe phone). By using the flash as the only illumination source on the sclera, the surface reflection component iskept constant for all images. This leaves the body reflection component and the camera sensor’s response as theonly two components that affect the sclera’s appearance. For the sake of this study, all images were capturedusing the same device, holding the camera sensor’s response constant and leaving the body reflection componentas the only variable left.The second accessory (Fig. 2, bottom) is a custom pair of paper glasses, reminiscent of the 3D glasses found atmovie theaters. The glasses have no lenses inside their frames. Along the rims of the glasses are various colored1https://www.d-eyecare.com/2https://vr.google.com/cardboard/ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies,Vol.1,No.2,Article20.Publicationdate: June 2017.
20:8 • Mariakakis et al.regions. The corners near the temples and the nose have smaller black squares surrounded by the glasses’ whitebackground. These squares act as fiducials, similar to those seen in QR codes. The rest of the regions along therims are the following colors (in no particular order): cyan, magenta, yellow, 17% gray, 33% gray, 50% gray, 67%gray, 83% gray, and black. The use of the colored squares is inspired by color calibration target cards like theMacbeth ColorChecker [29]. Rather than keeping the surface reflection component and the camera sensor’sresponse constant, the colored squares allow for all images to be normalized to the same references. The colorsalong the rims of the glasses are knowna priori. This means that their body reflection component is known andany deviation between their appearance and their true color is due to the surface reflection component and thecamera sensor’s response. Section 4.3 explains the calibration procedure that is used to define a calibration matrixthat best simulates the effects of the latter two color information components, which can later be applied to thesclerae themselves to reveal their true body reflection components.From a usability perspective, the glasses are more convenient for the user and cheaper to manufacture. However,the colors along the rims of the glasses must always be consistent, both across time and different pairs. If thecolors were to fade over time, the colors would become a changing reference that could lead to inaccurateresults. Although the box is bulkier, its requirements are far looser. The box’s main purpose is to block outambient lighting; control over the precise placement of the smartphone is convenient for aspects of the automaticsegmentation, but the box’s dimensions do not require as strict precision as the glasses’ colors.From a technical perspective, the color calibration procedure for the glasses can incur its own inaccuracies. InBiliScreen’s current state, though, the algorithm for the box accessory does not account for the camera sensor’sresponse. If users were to use a phone with a camera different from that of the iPhone SE, we can make noguarantee that colors will appear the same between the two. Section 6.1 discusses ways for addressing thislimitation. Even though the color calibration procedure for the glasses may introduce noise, it allows for anydevice to be used without issue. The calibration procedure captures the effects of both the surface reflectioncomponent and the camera sensor’s response.3.4 BiliScreen ApplicationAll data was collected by a research staff member through a custom app on an iPhone SE. The images collectedby the app were at a resolution of 1920×1080. The research staff member ensured that participants complied withthe procedure and noted any difficulties that participants had with the app and its accessories.The BiliScreen app developed for our study was designed to collect data for both accessories in a similarmanner. Before the use of either accessory, the smartphone’s flash was turned on. When using the box, the flashis necessary since it is the only way to make the eyes visible within it. Keeping the flash constantly on ratherthan bursting it at the time of the pictures was a consideration for participant comfort since the stark change inlighting can be unpleasant. When using the glasses, the flash was left on in case there was insufficient lighting inthe room or the glasses created a shadow on the participant’s face.After the flash was turned on, the research staff member placed the smartphone in the BiliScreen box. A holein the back of the box provided access to the screen for starting and stopping data collection. The app promptedthe participant to look in four different directions - up, left, right, and straight ahead - one at a time while takinga picture after each. Having the participant look in different directions exposed different parts of the sclera,some of which may have exhibited more jaundice than others. The participant was not asked to look downwardsince doing so covers their eyes with their eyelids. Once the pictures were taken inside the box, the researchstaff member removed the smartphone and held it approximately 0.5 m away from the participant’s face to takepictures with the glasses. This distance is roughly how far away we would expect participants to hold theirsmartphones if they were taking a selfie. The participant looked at each direction for two trials per accessory,yielding 2 BiliScreen accessories×2 trials per accessory×4 gaze directions per trial = 16 images per participant.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies,Vol.1,No.2,Article20.Publicationdate: June 2017.
BiliScreen: Smartphone-Based Scleral Jaundice Monitoring for Liver and Pancreatic Disorders • 20:94 BILISCREEN ALGORITHMFig. 3. The algorithm pipeline for both BiliScreen accessories. Images from both the box and the glasses go through the samesclera segmentation, feature extraction, and machine learning steps (with their own respective models and small parameterchanges). Images gathered with the glasses must go through the extra steps of glasses segmentation and color calibration.Fig. 3 outlines the high-level algorithm pipeline that transforms a BiliScreen image to a bilirubin estimate. Wewill provide further detail in this section on each of these steps, starting with the segmentation of various regionsof interest, the transformation of those regions into feature vectors, and finally the machine learning itself.4.1 Sclera SegmentationThe first step to segmenting the sclera from BiliScreen images is to define regions of interest where the sclerashould be located. One way to logically identify these regions would be to use Haar feature-based cascadeclassifiers [20,35] that are used in many applications that require eye detection. However, off-the-shelf eyedetectors sometimes failed because features around the eyes (e.g., eyebrows) were obstructed by the BiliScreenbox and glasses. To maintain consistency across images, regions of interest are defined through other methodsdepending on the BiliScreen accessory in use. Within the BiliScreen box, the regions of interest are defined asrectangular bounding boxes located on the left and right half side of the box using predetermined pixel offsetswithin the image. This is possible because the placement of the camera within the box is always the same. Theoffsets were defined such that the regions of interest would cover various face placements and inter-pupillarydistances. For the BiliScreen glasses, the regions of interest are more precisely defined as the regions surroundedby the colored squares (refer to Section 4.2 for how those squares are identified).Our approach to sclera segmentation relies on an algorithm called GrabCut [30], a technique for separatinga foreground object from its background; the terms “foreground” and “background” do not necessarily referto the perceivable foreground and background of the image, but rather a region of interest versus everythingelse in the image. GrabCut treats the pixels of an image as nodes in a graph. The nodes are connected by edgesthat are weighted according to the pixels’ spatial and chromatic similarity. Nodes in the graph are assignedone of four labels: definitely foreground, definitely background, possibly foreground, and possibly background.After initialization, graph cuts [6,13] are applied to re-assign node labels such that the energy of the graph isminimized. Normally, GrabCut is an interactive technique that is typically initialized with a bounding rectangleand then followed with user-drawn strokes that further clarify the object of interest. BiliScreen uses GrabCutwith a similar procedure, but without human intervention.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies,Vol.1,No.2,Article20.Publicationdate: June 2017.

